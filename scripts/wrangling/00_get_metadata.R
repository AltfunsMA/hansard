#! /usr/bin/Rscript
# Downloading the metadata CSV from aus parl
# Opens query session, stores cookies, and saves autogenerated CSV
# Alfonso Mart√≠nez Arranz
# R 3.6.1

# Set-up ----
pacman::p_load(lubridate, tidytable, tidyverse, progress)

cargs <- commandArgs(trailingOnly = T)

attempt <- cargs[1]


# URLs----

basic_url <- "https://parlinfo.aph.gov.au/parlInfo/"

csv_prefix <- "csv/csv.w3p;"

summary_prefix <- "search/summary/summary.w3p;adv=yes;"

dsets_suffix <- "%20Dataset%3Ahansardr,hansardr80,hansardrIndex,hansards,hansards80,hansardsIndex;resCount=Default"

# Dates ---

num_months <- interval(ymd("1901-01-01"), ymd("2021-11-30")) %/% months(1)


starts <-  ymd("1901-01-01") %m+% months(1:num_months)

ends <- ymd("1901-01-31") %m+% months(1:num_months)


zeropad <- function(d) str_pad(d, width = 2, side = "left", "0")

# Filepaths ---- 

general_path <- "/data/hansard/general/"

success_path <- paste0(general_path, "/01_records/")

out_path <- paste0(general_path, "tmp/")

cookie_path <- paste0(out_path, "latest_cookie.txt")

html_path <- paste0(out_path, "latest_page.html")

temp_log_path <- "/data/hansard/logs/dl_metadata_codes.log"

main_log_path <- "/data/hansard/logs/get_hans_metadata.log"

latest_http_log <- fread.(temp_log_path, sep = "\t", col.names = c("period", "csv"),
                          fill = TRUE, )

already_done <- str_remove_all(list.files(success_path, "\\.csv"), 'metadata_|\\.csv')

main_log <- read_lines(main_log_path)

main_log <- unique(c(main_log, already_done))

if(length(latest_http_log) > 1) {
  
new_logs <- latest_http_log %>% 
  mutate_all(~str_extract(.x, "[0-9-_]+")) %>%
  filter(csv == 200) %>%
  pull(period)
  
  
  all_logs <- unique(c(main_log, new_logs))
  

} else {
  
  log_created <- file.create(temp_log_path)
  
  if(log_created) message("New templog file created.")
  
  all_logs <- main_log
  
}
  
write_lines(all_logs, main_log_path)



# Main ----

len <- length(starts)

pb = progress_bar$new(format = "  downloading [:bar] :percent eta: :eta",
                      total = len) 

for (i in 1:len) {
   
  pb$tick(0)
  
  # Check existing ---
  
  period <- paste0(starts[i], "_", ends[i])
  
  pb$tick()
  
  if(period %in% all_logs) {
    
    next
    
  } 
  
  write_lines(period, sep = "\t", temp_log_path, append = T)
  
  # generate URLs and commands
  
  smonth <- zeropad(month(starts[i]))
  syear <- year(starts[i])
  eday <- zeropad(day(ends[i]))
  emonth <- zeropad(month(ends[i]))
  eyear <- year(ends[i])
  
  date_infix <- paste0("orderBy=customrank;page=0;query=Date%3A01%2F", 
                       smonth, "%2F", syear, 
                       "%20%3E%3E%20",
                       eday, "%2F", emonth, "%2F", eyear)
  
  
  summary_full_url <- paste0('"', basic_url, summary_prefix, date_infix, dsets_suffix, '"')
  csv_full_url <- paste0('"', basic_url, csv_prefix, date_infix, dsets_suffix, '"')
  

  
  save_path <- paste0(out_path, "metadata_", period, ".csv")

  summary_cmd <- 
    paste("wget -q --no-check-certificate --keep-session-cookies --save-cookies", 
        cookie_path,
        "-O",
        html_path,
        summary_full_url, "2>&1 /dev/null")
  
  if(i == 1) cat("Sample summary query: \n", summary_cmd, "\n")
  
  csv_cmd <- paste("wget --no-check-certificate --load-cookies", cookie_path, "-O", 
                   save_path, 
                   csv_full_url, "2>&1 | egrep -i 'HTTP request sent' >>", temp_log_path)
  
  
  if(i == 1) cat("Sample CSV query: \n", csv_cmd, "\n")
  
  
  

  # Run queries and store into temp_log_path ---- 
    
  
  system(summary_cmd)
  
  Sys.sleep(as.numeric(attempt) + 1)
  
  system(csv_cmd)

  Sys.sleep(1)
  
  mv_success <- paste('find', out_path, 
                      '-maxdepth 1 -size +100 -type f -name "*.csv" -exec mv -t',
                      success_path, '{} +')
  
  system(mv_success)
  

  
}


message("Set of loops DONE")









